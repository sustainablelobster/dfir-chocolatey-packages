<?xml version="1.0" encoding="utf-8"?>
<!-- Do not remove this test for UTF-8: if “Ω” doesn’t appear as greek uppercase omega letter enclosed in quotation marks, you should use an editor that supports UTF-8, not this one. -->
<package xmlns="http://schemas.microsoft.com/packaging/2015/06/nuspec.xsd">
  <metadata>
    <id>logfileparser</id>
    <version>2.0.0.51</version>
    <packageSourceUrl>https://github.com/sustainablelobster/dfir-chocolatey-packages/tree/master/logfileparser</packageSourceUrl>
    <owners>sustainablelobster</owners>
    <title>LogFileParser</title>
    <authors>Joakim Schicht </authors>
    <projectUrl>https://github.com/jschicht/LogFileParser</projectUrl>
    <copyright>2022 Joakim Schicht</copyright>
    <licenseUrl>https://github.com/jschicht/LogFileParser/blob/master/LICENSE.md</licenseUrl>
    <requireLicenseAcceptance>false</requireLicenseAcceptance>
    <projectSourceUrl>https://github.com/jschicht/LogFileParser</projectSourceUrl>
    <docsUrl>https://github.com/jschicht/LogFileParser/blob/master/readme.txt</docsUrl>
    <bugTrackerUrl>https://github.com/jschicht/LogFileParser/issues</bugTrackerUrl>
    <tags>logfileparser logfile parser ntfs digital forensics incident response dfir</tags>
    <summary>Parser for $LogFile on NTFS</summary>
    <description><![CDATA[## Background
NTFS is designed as a recoverable filesystem. This done through logging of all transactions that alters volume structure. So any change to a file on the volume will require something to be logged to the $LogFile too, so that it can be reversed in case of system failure at any time. Therefore a lot of information is written to this file, and since it is circular, it means new transactions are overwriting older records in the file. Thus it is somewhat limited how much historical data can be retrieved from this file. Again, that would depend on the type of volume, and the size of the $LogFile. On the systemdrive of a frequently used system, you will likely only get a few hours of history, whereas an external/secondary disk with backup files on, would likely contain more historical information. And a 2MB file will contain far less history than a 256MB one. So in what size range can this file be configured to? Anything from 256 KB and up. Configure the size to 2 GB can be done like this, "chkdsk D: /L:2097152". How a large sized logfile impacts on performance is beyond the scope of this text. Setting it lower than 2048 is normally not possible. However it is possble by patching untfs.dll: http://code.google.com/p/mft2csv/wiki/Tiny_NTFS

## Features

- Decode and dump $LogFile records and transaction entries.
- Decode NTFS attribute changes.
- Optionally resolve all datarun list information available in $LogFile. Option: "Reconstruct data runs".
- Recover transactions from slack space within $LogFile.
- Choose to reconstruct missing or damaged headers of transactions found in slack. Option: "Rebuild header". 
- Optionally also finetune result with a LSN error level value. Option: "LSN error level".
- Logs to csv and imports to sqlite database with several tables.
- Optionally import csv output of mft2csv into db.
- Choose among 6 different timestamp formats.
- Choose timestamp precision: None, MilliSec and NanoSec.
- Choose Precision separator at millisec.
- Choose Precision separator at nanosec.
- Choose region adjustment for timestamps. Default is to present timestamps in UTC 0.0.
- Choose output separator. Option: "Set separator".
- Configurable UNICODE or ANSI output. Option "Unicode".
- Configurable MFT record size (1024 or 4096). Option "MFT record size".
- Optionally decode individual transactions or partial transactions (fragment).
- Option to reconstruct RCRD's from single or multiple transactions (fragments).
- Option to configure broken $LogFile. Useful with carved RCRD's as input.
- Option to skip fixups (for broken $LogFile, typically carved from memory).
- Detailed verbose output into debug.log.
- Configurable comma separated list of lsn's to trigger ultra verbose information about specific transactions into debug.log.
- Configuration for 32-bit OS.
- Configuration for binary data extraction of resident data updates.
- Autogenerated sql for importing out put into MySql database.
- Option to skip all sqlite3 stuff to speed up total parsing.
- Optional command line mode. Supports errorlevel suitable for batch scripting.

## Note on VirusTotal detections

This application is a compiled [AutoIt](https://www.autoitscript.com/site/) script.
AutoIt scripts packaged in this way are often incorrectly flagged as malware.
This is issue is documented in the AutoIt Wiki here: https://www.autoitscript.com/wiki/AutoIt_and_Malware
]]></description>
    <releaseNotes>https://github.com/jschicht/LogFileParser/blob/master/changelog.txt</releaseNotes>
  </metadata>
  <files>
    <file src="tools\**" target="tools" />
  </files>
</package>
